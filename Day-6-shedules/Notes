There are 5 types of shedules are present in kubernetes
1. Daemonset
2. NodeSelector
3. Affinity-preferred
4. Affinity-required
5. Taint Tolerations


1. DaemonSet = 
      A DaemonSet ensures that one pod runs on every eligible node in a Kubernetes cluster. It is mainly used for node-level services rather than app workloads.

How DaemonSet scheduling works =
  Automatic placement
      1.When a DaemonSet is created, Kubernetes schedules one pod per matching node.
      2.When a new node joins the cluster, the pod is automatically added.
      3.When a node is removed, the pod is automatically deleted.

Practicle =
1. Create EKS and create nodegroup and create ec2 server for connect to nodegroup, configure aws credentials and aws eks update-kubeconfig --region us-east-1 --name <cluseter_name>
2. Create daemon.yml file and apply it --> kubectl apply -f daemon.yml --> kubectl get pods
3. You will see pod will create in each running nodes if you delete any pod it will create immediately.
4. Go to Auto scaling and go to edit option in desired capacity increase the count.
5. You will see new node will create and check the pods new pod will create, the desired capacity deponds on nodes if you delete node pod will delete.


*************************************************************************************************************************************************************************************


2. NodeSelector = 
    nodeSelector is a simple way to control which nodes a pod can run on using node labels.

**What nodeSelector does
      1.It tells Kubernetes: run this pod only on nodes with specific labels
      2.It uses exact key–value matching
      3.If no node matches, the pod stays in Pending state

**How it works
    1.Nodes have labels (key=value)
    2.Pod spec includes a nodeSelector
    3.Scheduler places the pod only on matching nodes


Practicle =
1. Create Nodeselector.yml file and apply it --> kubectl apply -f Nodeselector.yml --> At end we add Nodeselector and label-key and label-value
2. check pods created or not --> kubectl get pods --> you will see pods are in pending state because we not labeled the nodes.
3. To label the node use this command -->> kubectl label nodes <node-name> <label-key>=<label-value>
4. For finding the node name use command --> kubectl get pods -o wide --> check in Node option ip-ec2.internal is there so that is node name. 
5. Also you can check node name in running ec2 instance page select any node you will see instance summery in which check Hostname type you will see node name e.g ip-172-31-34-126.ec2.internal
6. kubectl label nodes  ip-172-31-34-126.ec2.internal clr=pink
7. Then check pods --> kubectl get pods --> all the pending pods will be in running state.
8. For delabel node use command -->  kubectl label nodes <node-name> <label-key>-  e.g  kubectl label nodes ip-172-31-34-126.ec2.internal clr-
9. If you delabel the node still pod is running because at the schedule time it will select the node with label name mentioned in yml file.
10. kubectl get nodes --show-labels  ### To check labels of the nodes
11.Also if we remove nodeselector lines from the yml file the pod are unlabeled then we run the file the unlabeled pods create in labeld node.
12.But labeled pod is not run on unlabled nodes.


***********************************************************************************************************************************************



3. Node Affinity Preffered =
      Preferred node affinity lets you guide pod placement without forcing it. 
      Kubernetes will try to schedule the pod on matching nodes, but if none are available, it will still run the pod elsewhere.

**What “preferred” means
      1. Soft rule, not mandatory
      2. Scheduler gives priority to matching nodes
      3. Pod will not stay Pending if no node matches
      4. Uses weights to decide preference strength

Preferred affinity uses:
      preferredDuringSchedulingIgnoredDuringExecution


Practicle =
1. Create affinity-preffered.yml file and apply it.
2. kubectl get pods -o wide --> You will see pods are running but we did not labeled any nodes.
3. Preferred node affinity works as follows: if the node labels match the pod’s affinity rules, the pods will be scheduled on the labeled nodes.
4. If the pod’s preferred affinity labels do not match any node labels, the pods will still be created and can run on any available node.
5. Label the nodes --> kubectl label nodes <node_name> <key>=<value>
6. Apply the yml file and then --> kubectl get pods -o wide --> you will see all the pod created on same nodes.

***************************************************************************************************************************************************


4. Affinity Required
      Required node affinity is used when a pod must run only on specific nodes. If no node matches, the pod will not be scheduled.

**What “required” means
      1.Hard rule, mandatory
      2.Pod runs only on matching nodes
      3.If no node matches, pod stays Pending
      4.No weights involved

The field used is:
      requiredDuringSchedulingIgnoredDuringExecution

**How scheduling works
      1.Scheduler checks all nodes
      2.Nodes that do not match are rejected
      3.Pod is placed only on a matching node
      4.If matching nodes are removed later, the pod is not evicted

Practicle =
1. Create affinity-required.yml file apply file
2. kubectl get pods -o wide --> pods are in pending state because labels are not matching with nodes.
3. label the nodes --> kubectl label <node-name> <label-key>=<label-value> --> kubectl label nodes  ip-172-31-34-126.ec2.internal size=medium
4. check pod status --> kubectl get pods --> all pods are running in same nodes.
5. Required affinity works as: if the node labels match the pod’s affinity rules, the pods will be scheduled on the labeled nodes.
6. If the pod’s required affinity labels do not match any node labels, the pods will still be in pending state.

******************************************************************************************************************************************


****NodeSelector Vs Affinity Required 

##nodeSelector = 
      1.Simple way to schedule pods on labeled nodes
      2.Uses exact key = value match
      3.All conditions are AND
      4.Very easy to configure
      5.Limited flexibility

##Required node affinity =
      1.Advanced scheduling control
      2.Uses matchExpressions
      3.Supports In, NotIn, Exists, Gt, Lt
      4.Can express OR conditions
      5.More flexible and recommended

| Feature         | nodeSelector    | Required node affinity    |
| --------------- | --------------- | ------------------------- |
| Rule type       | Hard            | Hard                      |
| Match type      | Exact match     | Expression-based          |
| Operators       | Not supported   | In, NotIn, Exists, Gt, Lt |
| OR condition    | ❌ No            | ✅ Yes                  |
| Complexity      | Very simple     | Moderate                  |
| Recommended for | Basic use cases | Production workloads      |


***************************************************************************************************************************************


5. Taint Toleration =  Watch video 2025-12-17 for more details
By default, the scheduler can place any pod on any node.
Taints and tolerations let you control this, so only specific pods can run on specific nodes.

Think of it as:
      Taint on node: “Do not schedule pods here”
      Toleration on pod: “I am allowed to run here”


1.Taints (Node side)
      A taint is applied to a node.

Effects
      1.NoSchedule
      Pods without matching toleration will not be scheduled.
      
      2.PreferNoSchedule
      Scheduler avoids the node if possible.
      
      3.NoExecute
      Existing pods without toleration are evicted, new ones are blocked.

e.g  kubectl taint nodes node1 env=prod:NoSchedule   #### Only pods that tolerate env=prod can run on node1.


2. Tolerations (Pod side)
      A toleration is added to a pod spec.

toleration.yml
tolerations:
- key: "env"
  operator: "Equal"
  value: "prod"
  effect: "NoSchedule"

This pod can be scheduled on nodes tainted with env=prod:NoSchedule.

**Important points
      Tolerations do not force pods onto tainted nodes.
      They only allow scheduling there.
      
      To force placement, use nodeSelector or node affinity along with tolerations.
      
      Taints protect nodes. Tolerations are permissions, not rules.


Practicle =
1. create toleration.yml file and taint the node.
2. kubectl taint nodes <node-name> <key>=<value>:NoSchedule       ### command for taint the node 
3. kubectl taint nodes <node-name> <key>=<value>:NoSchedule-      ### command for untaint the node
4. In our case command is --> kubectl taint nodes ip-172-31-12-169.ec2.internal app=blue:NoSchedule
5. apply toleration file --> kubectl apply -f toleration.yml 
6. kubectl get pods -o wide --> you will see the pods are runnig on tainted node because toleration pods run on tainted nodes.
7. Delete the pods --> kubectl delete -f .
8. Make some changes in yml file remove the toleration block and apply file --> kubectl apply -f .
9. kubectl get pods -o wide --> you will see all the pods are running in non taineted node.
10. If you taint both the nodes with NoSchedule and remove toleration block from file then you apply file the pods will go in pending state.
11. Untaint both the nodes --> kubectl taint nodes ip-172-31-12-169.ec2.internal app=blue:NoSchedule-
12. Taint any one node with NoExecute --> kubectl taint nodes ip-172-31-12-169.ec2.internal app=blue:NoExecute
13. Add toleration block in yml file with NoSchedule and apply file
14. You will see pod will run on non tainted node because NoExecute will not allow toleration pods.
15. change yml file and in toleration block NoSchedule replace with NoExcute block and apply file
16. kubectl get pods -o wide --> you will see all the pods are running in NoExcute tainted node.
17. Untaint the node and taint with NoSchedule
18. Do changes in yml file replace NoExecute with NoSchedule and apply file.
19. Check pods you will see pods running on noschedule node.
20. Then untaint that node and taint with NoExecute and check the pods all the running pods will be deleted.

Possible real time Scenarios of Taint and Toleration
1. If a pod has a NoSchedule toleration and the node has a NoSchedule taint, the pod will run on the node.
2. If a pod has no toleration and the node has a NoSchedule taint, the pod will remain in the Pending state.
3. If a pod has a NoSchedule toleration and the node has a NoExecute taint, the pod will be evicted or will not stay running.
4. If a pod has a NoExecute toleration and the node has a NoExecute taint, the pod will run on the node.
5. If a pod is running due to a NoSchedule toleration and the node is later tainted with NoExecute, the pod will be deleted unless it has a NoExecute toleration.

Pod has NoSchedule toleration and node has NoSchedule taint → pod runs on the node.

Pod has no toleration and node has NoSchedule taint → pod stays in Pending state.

Pod has PreferNoSchedule toleration and node has NoSchedule taint → pod stays in Pending state.

Pod has NoExecute toleration and node has NoSchedule taint → pod stays in Pending state.

Pod has no toleration and node has PreferNoSchedule taint → pod may run if no other nodes are available.

Pod has PreferNoSchedule toleration and node has PreferNoSchedule taint → pod runs on the node.

Pod has NoSchedule toleration and node has PreferNoSchedule taint → pod runs on the node.

Pod has NoExecute toleration and node has PreferNoSchedule taint → pod runs on the node.

Pod has no toleration and node has NoExecute taint → pod is evicted or will not stay running.

Pod has NoSchedule toleration and node has NoExecute taint → pod is evicted or will not stay running.

Pod has PreferNoSchedule toleration and node has NoExecute taint → pod is evicted or will not stay running.

Pod has NoExecute toleration and node has NoExecute taint → pod runs on the node.

Pod has NoExecute toleration with tolerationSeconds and node has NoExecute taint → pod runs for that duration and then is evicted.

Pod toleration without effect specified and matching key or value → pod tolerates NoSchedule, PreferNoSchedule, and NoExecute taints.

Pod toleration with operator Equal → pod tolerates only the exact key and value.

Pod toleration with operator Exists and key specified → pod tolerates any value of that key.

Pod toleration with operator Exists and no key → pod tolerates all taints on all nodes.

Node has multiple taints → pod must tolerate all taints to run on that node.

Pod has multiple tolerations → pod can run on any node whose taints are all tolerated.

NoSchedule taint added after pod is running → existing pod continues running.

NoExecute taint added after pod is running → pod is evicted if it lacks NoExecute toleration.

DaemonSet pods automatically tolerate not-ready and unreachable taints.

kubectl drain applies NoExecute taint and evicts non-DaemonSet pods.

Pod has toleration only → pod may run on tainted or untainted nodes.

Pod has toleration plus nodeSelector or nodeAffinity → pod runs only on the selected tainted node.
