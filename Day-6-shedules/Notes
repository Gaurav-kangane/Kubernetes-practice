There are 5 types of shedules are present in kubernetes
1. Daemonset
2. NodeSelector
3. Affinity-preferred
4. Affinity-required
5. Taint Tolerations


1. DaemonSet = 
      A DaemonSet ensures that one pod runs on every eligible node in a Kubernetes cluster. It is mainly used for node-level services rather than app workloads.

How DaemonSet scheduling works =
  Automatic placement
      1.When a DaemonSet is created, Kubernetes schedules one pod per matching node.
      2.When a new node joins the cluster, the pod is automatically added.
      3.When a node is removed, the pod is automatically deleted.

Practicle =
1. Create EKS and create nodegroup and create ec2 server for connect to nodegroup, configure aws credentials and aws eks update-kubeconfig --region us-east-1 --name <cluseter_name>
2. Create daemon.yml file and apply it --> kubectl apply -f daemon.yml --> kubectl get pods
3. You will see pod will create in each running nodes if you delete any pod it will create immediately.
4. Go to Auto scaling and go to edit option in desired capacity increase the count.
5. You will see new node will create and check the pods new pod will create, the desired capacity deponds on nodes if you delete node pod will delete.


*************************************************************************************************************************************************************************************


2. NodeSelector = 
    nodeSelector is a simple way to control which nodes a pod can run on using node labels.

**What nodeSelector does
      1.It tells Kubernetes: run this pod only on nodes with specific labels
      2.It uses exact key–value matching
      3.If no node matches, the pod stays in Pending state

**How it works
    1.Nodes have labels (key=value)
    2.Pod spec includes a nodeSelector
    3.Scheduler places the pod only on matching nodes


Practicle =
1. Create Nodeselector.yml file and apply it --> kubectl apply -f Nodeselector.yml --> At end we add Nodeselector and label-key and label-value
2. check pods created or not --> kubectl get pods --> you will see pods are in pending state because we not labeled the nodes.
3. To label the node use this command -->> kubectl label nodes <node-name> <label-key>=<label-value>
4. For finding the node name use command --> kubectl get pods -o wide --> check in Node option ip-ec2.internal is there so that is node name. 
5. Also you can check node name in running ec2 instance page select any node you will see instance summery in which check Hostname type you will see node name e.g ip-172-31-34-126.ec2.internal
6. kubectl label nodes  ip-172-31-34-126.ec2.internal clr=pink
7. Then check pods --> kubectl get pods --> all the pending pods will be in running state.
8. For delabel node use command -->  kubectl label nodes <node-name> <label-key>-  e.g  kubectl label nodes ip-172-31-34-126.ec2.internal clr-
9. If you delabel the node still pod is running because at the schedule time it will select the node with label name mentioned in yml file.
10. kubectl get nodes --show-labels  ### To check labels of the nodes
11.Also if we remove nodeselector lines from the yml file the pod are unlabeled then we run the file the unlabeled pods create in labeld node.
12.But labeled pod is not run on unlabled nodes.


***********************************************************************************************************************************************



3. Node Affinity Preffered =
      Preferred node affinity lets you guide pod placement without forcing it. 
      Kubernetes will try to schedule the pod on matching nodes, but if none are available, it will still run the pod elsewhere.

**What “preferred” means
      1. Soft rule, not mandatory
      2. Scheduler gives priority to matching nodes
      3. Pod will not stay Pending if no node matches
      4. Uses weights to decide preference strength

Preferred affinity uses:
      preferredDuringSchedulingIgnoredDuringExecution


Practicle =
1. Create affinity-preffered.yml file and apply it.
2. kubectl get pods -o wide --> You will see pods are running but we did not labeled any nodes.
3. Preferred node affinity works as follows: if the node labels match the pod’s affinity rules, the pods will be scheduled on the labeled nodes.
4. If the pod’s preferred affinity labels do not match any node labels, the pods will still be created and can run on any available node.
5. Label the nodes --> kubectl label nodes <node_name> <key>=<value>
6. Apply the yml file and then --> kubectl get pods -o wide --> you will see all the pod created on same nodes.

***************************************************************************************************************************************************


4. Affinity Required
      Required node affinity is used when a pod must run only on specific nodes. If no node matches, the pod will not be scheduled.

**What “required” means
      1.Hard rule, mandatory
      2.Pod runs only on matching nodes
      3.If no node matches, pod stays Pending
      4.No weights involved

The field used is:
      requiredDuringSchedulingIgnoredDuringExecution

**How scheduling works
      1.Scheduler checks all nodes
      2.Nodes that do not match are rejected
      3.Pod is placed only on a matching node
      4.If matching nodes are removed later, the pod is not evicted

Practicle =
1. Create affinity-required.yml file apply file
2. kubectl get pods -o wide --> pods are in pending state because labels are not matching with nodes.
3. label the nodes --> kubectl label <node-name> <label-key>=<label-value> --> kubectl label nodes  ip-172-31-34-126.ec2.internal size=medium
4. check pod status --> kubectl get pods --> all pods are running in same nodes.
5. Required affinity works as: if the node labels match the pod’s affinity rules, the pods will be scheduled on the labeled nodes.
6. If the pod’s required affinity labels do not match any node labels, the pods will still be in pending state.

******************************************************************************************************************************************


****NodeSelector Vs Affinity Required 

##nodeSelector = 
      1.Simple way to schedule pods on labeled nodes
      2.Uses exact key = value match
      3.All conditions are AND
      4.Very easy to configure
      5.Limited flexibility

##Required node affinity =
      1.Advanced scheduling control
      2.Uses matchExpressions
      3.Supports In, NotIn, Exists, Gt, Lt
      4.Can express OR conditions
      5.More flexible and recommended

| Feature         | nodeSelector    | Required node affinity    |
| --------------- | --------------- | ------------------------- |
| Rule type       | Hard            | Hard                      |
| Match type      | Exact match     | Expression-based          |
| Operators       | Not supported   | In, NotIn, Exists, Gt, Lt |
| OR condition    | ❌ No            | ✅ Yes                     |
| Complexity      | Very simple     | Moderate                  |
| Recommended for | Basic use cases | Production workloads      |
